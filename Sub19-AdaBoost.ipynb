{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textit{Indra Ikauniece}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboosting is an ensemble method that creates a strong classifier from a number of week classifiers. A week classifier performs poorly, but better than guessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most suited and therefore most common algorithm used with AdaBoost are decision trees with one level. Because these trees are so short and only contain one decision for classification, they are often called decision stumps. But Adaboost can be applied to any classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost trains several weak classifiers on different random subsets of training data. After each training Adaboost assigns a weight to each traing data point, so that data points with higher weights will have a higher probability of being included in the next training subset. From each training, the examples that were misclassified, are assigned higher weight, so they will have a higher probability of being corrected in the next training. (Before the first training all data points are assigned equal probability.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the classidiers are trained, they are each given weights based on their accuracies. Classifiers with 50% accuracies are assigned weight zero (because 50% accuracy is the same as guessing), classifiers with higher than 50% accuracies are assigned weights larger than zero, and classifiers with less than 50% accuracies are assigned negative weights (because they predict the opposite of correct). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_0(x_i)=1/N$ - weights before training the first model\n",
    "\n",
    "Create classifier $h_j(x), i=j,..,K$ from the training subset.\n",
    "\n",
    "Then compute the output weight for that classifier: $\\alpha_j=\\frac{1}{2}\\ln\\big(\\frac{1-\\epsilon_{j}}{ \\epsilon_{j}}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot of $\\alpha$ we can see that at $0.5$ error rate, the assigned weight is $0$, and then for the error rate larger than $0.5$ the assigned weight is positive, and for the error rate less than $0.5$ the assigned weight is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](adaboost_alphacurve.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the classfier weight, we then compute weights for each training data point. \n",
    "\n",
    "$\n",
    "w_j(x_i)=\\frac{w_{j-1}(x_i) e^{- y_i \\alpha_j h_j(x_i)}}{Z_j}\n",
    "$\n",
    "\n",
    "$w_j$ is a vector of weights for each point in the training data set. \n",
    "\n",
    "$Z_j=\\sum_i w_j(x_i)$ is the sum of all weights, used to normalize the weights so that they sum up to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Adaboost on Churn problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ok.\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "#Recover Churn data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"files/churn_curated_numerical.csv\",header=None)\n",
    "df.head()\n",
    "data = df.values\n",
    "X = data[:,:-1]\n",
    "y = 2*data[:,-1]-1\n",
    "print ('Loading ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First predict churn with a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90969096909690972"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X, y)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the same problem with decision trees using Adaboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91179117911791174"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=0),algorithm=\"SAMME\",n_estimators=200)\n",
    "clf.fit(X, y)\n",
    "scores = cross_val_score(clf, X, y)\n",
    "scores.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a small $1\\%$ improvement in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
